{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599776990609",
   "display_name": "Python 3.6.9 64-bit ('lasalle': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio: Regresion logística usando numpy\n",
    "Este es el notebook correspondiente al laboratorio de regresión logística. Usted deberá implementar el algoritmo en base al notebook de la [regresión lineal](../regresion_lineal/lin_reg.ipynb).\n",
    "\n",
    "## Instrucciones\n",
    "Deberá implementar y completar código faltante correspondiente a las siguientes funciones:\n",
    "  - **Función sigmoide**: $g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "  - **Hipótesis**: $h(X) = g(\\theta^T * X)$\n",
    "  - **Verosimilitud Logaritmica**: $l(\\theta) = \\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log{h(x^{(i)}) + (1 - y^{(i)})(1 - \\log{h(x^{(i)})})}$\n",
    "  - **Gradiente de la Verosimilitud**: $\\nabla l(\\theta) = \\frac{1}{m} (y - h(X)) *  X^T$\n",
    "\n",
    "También deberá completar el código faltante para la inicialización y la implementación del **ascenso de gradiente**\n",
    "\n",
    "Modifique los **hiperpámetros** $\\alpha$ y el número de iteraciones para obtener un resultado adecuado.\n",
    "\n",
    "**Nota:** Solo deberá modificar porciones de código marcadas con:\n",
    "\n",
    "```python\n",
    "###-----------------------\n",
    "### CODIGO DEL ESTUDIANTE\n",
    "###-----------------------\n",
    "```\n",
    "\n",
    "## Evaluación\n",
    "El algoritmo implementado deberá ser capaz de entrenar con éxito un modelo de regresión logística para clasificación binaria, se comparará el valor de la función $l(\\theta)$ inicial con el valor final, debiendo el valor final ser mayor al inicial ya que se intenta maximizar la Verosimilitud a través del ascenso de gradiente.\n",
    "\n",
    "Use las gráficas de la **curva de aprendizaje** para analizar la evolución de la función $l(\\theta)$ durante el entrenamiento. La curva de aprendizaje deberá converger en un valor mayor al inicial.\n",
    "\n",
    "## Recomendaciones\n",
    "Ejecute el notebook de la regresión lineal de principio a fin y asegúrese de comprender bien cada celda antes de intentar implementar la regresión logística. Gran parte de este laboratorio está basado en la implementación de la regresión lineal.\n",
    "\n",
    "Realice pruebas de consistencia de cada función implementada en una sesión interactiva para garantizar el funcionamiento correcto. Es útil realizar un análisis de las dimensiones de las matrices utilizadas.\n",
    "\n",
    "Use el código disponible en el notebook de la regresión lineal como referencia. La mayoría del código de este ejercicio puede ser extraído y modificado fácilmente en base al notebook anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_classification_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para la regresión logística\n",
    "\n",
    "> IMPLEMENTAR FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    'Funcion sigmoide'\n",
    "    ###-----------------------\n",
    "    ### CODIGO DEL ESTUDIANTE\n",
    "    ###-----------------------\n",
    "\n",
    "\n",
    "def h(X, theta):\n",
    "    'Hipotesis de la regresion Logistica, devuelve una matriz de (1,m)'\n",
    "    ###-----------------------\n",
    "    ### CODIGO DEL ESTUDIANTE\n",
    "    ###-----------------------\n",
    "\n",
    "\n",
    "def l(X, y, theta):\n",
    "    'Funcion de verosimilitud logaritmica de la regresion logistica, devuelve un escalar'\n",
    "    ###-----------------------\n",
    "    ### CODIGO DEL ESTUDIANTE\n",
    "    ###-----------------------\n",
    "\n",
    "def dl(X, y, theta):\n",
    "    'Gradiente de la verosimilitud con respecto a theta, devuelve una matriz de (n + 1, 1)'\n",
    "    ###-----------------------\n",
    "    ### CODIGO DEL ESTUDIANTE\n",
    "    ###-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar y preprocesar el dataset\n",
    "Para poder entrenar la regresion lineal de forma exitosa, se debe realizar un procesamiento previo (normalizacion)\n",
    "> NO MODIFICAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset\n",
    "X, y = load_classification_data()\n",
    "\n",
    "m = X.shape[1]  # Numero de ejemplos\n",
    "n = X.shape[0]  # Numero de caracteristicas\n",
    "\n",
    "# dimensiones\n",
    "print(f'X:{X.shape}, y: {y.shape}')\n",
    "\n",
    "# Normalizar los vectores de entrada y salida\n",
    "X = (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "\n",
    "# Agregar una fila de 1 al inicio del dataset\n",
    "unos = np.ones((1, m))\n",
    "\n",
    "X = np.append(unos, X, axis=0)\n",
    "print(f'X_:{X.shape}, y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "Primero debemos inicializar los hiperparametros y luego ejecutar el bucle de entrenamiento usando descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparametros\n",
    "Los hiperparametros son parametros o valores que podemos controlar pero que no son los parametros del modelo predictivo ($\\Theta$)\n",
    "> ASIGNAR VALORES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ### CODIGO DEL ESTUDIANTE\n",
    "iteraciones = ### CODIGO DEL ESTUDIANTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializacion de los parametros de la regresion\n",
    "Se recomienda inicializar los parametros $\\Theta$ de forma aleatoria.\n",
    "> IMPLEMENTAR CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta = ### CODIGO DEL ESTUDIANTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registro del entrenamiento\n",
    "En cada iteracion del descenso de gradiente vamos a almacenar los valores de la funcion de costo y los gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "registro_grads = []\n",
    "registro_l = []\n",
    "l_init = l(X, y, theta)\n",
    "print(f'Verosimilitud inicial: {l_init}')\n",
    "registro_l.append(l_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ascenso de gradiente\n",
    "> IMPLEMENTAR CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(iteraciones):\n",
    "    ###-----------------------\n",
    "    ### CODIGO DEL ESTUDIANTE\n",
    "    ###-----------------------\n",
    "\n",
    "print(f'Verosimilitud final: {l(X, y, theta)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "it = list(range(iteraciones + 1))\n",
    "plt.plot(it, registro_l)\n",
    "plt.title('Curva de aprendizaje')\n",
    "plt.show()"
   ]
  }
 ]
}